{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor Critic implementation\n",
    "\n",
    "An example implementation by http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udtLxTQyiNUz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tensorflow version', '1.15.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as keras_backend\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\"tensorflow version\", tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "nfKWiIG6m5Ed",
    "outputId": "e0de31b2-69a4-48dc-aba0-7e77c3fab5e3"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4X3S4uD_m-e_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAZ_YRduimE0"
   },
   "outputs": [],
   "source": [
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits, **kwargs):\n",
    "        # Sample a random categorical action from the given logits.\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "    \n",
    "    \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # Note: no tf.get_variable(), just simple Keras API!\n",
    "        self.hidden1 = kl.Dense(128, activation='relu')\n",
    "        self.hidden2 = kl.Dense(128, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # Logits are unnormalized log probabilities.\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Inputs is a numpy array, convert to a tensor.\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        # Separate hidden layers from the same input tensor.\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # Executes `call()` under the hood.\n",
    "        # Should be model.predict instead of predict_on_batch\n",
    "        # as it is deprecated\n",
    "        logits, value = self.predict_on_batch(obs)\n",
    "        action = self.dist.predict_on_batch(logits)\n",
    "        # Another way to sample actions:\n",
    "        #   action = tf.random.categorical(logits, 1)\n",
    "        # Will become clearer later why we don't use it.\n",
    "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful example of how to debug RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vk4Phsi1tvS-",
    "outputId": "ae0ed46f-e14f-4c64-de95-442e9280b635"
   },
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# model = Model(num_actions=env.action_space.n)\n",
    "\n",
    "# obs = env.reset()\n",
    "# # No feed_dict or tf.Session() needed at all!\n",
    "# action, value = model.action_value(obs[None, :])\n",
    "# print(action, value) # [1] [-0.00145713]\n",
    "\n",
    "# class A2CAgent:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def test(self, env, render=True):\n",
    "#         obs, done, ep_reward = env.reset(), False, 0\n",
    "#         while not done:\n",
    "#             action, _ = self.model.action_value(obs[None, :])\n",
    "#             obs, reward, done, _ = env.step(action)\n",
    "#             ep_reward += reward\n",
    "#             if render:\n",
    "#                 env.render()\n",
    "#         return ep_reward\n",
    "\n",
    "\n",
    "# agent = A2CAgent(model)\n",
    "# rewards_sum = agent.test(env)\n",
    "# print(\"%d out of 200\" % rewards_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class A2C:\n",
    "    '''Advantage Actor Critic Agent'''\n",
    "    def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "        # Coefficients are used for the loss terms.\n",
    "        self.value_c = value_c\n",
    "        self.entropy_c = entropy_c\n",
    "        \n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "          optimizer=ko.RMSprop(lr=lr),\n",
    "          # Define separate losses for policy logits and value estimate.\n",
    "          loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "    def test(self, env, render=False):\n",
    "        obs, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(obs[None, :])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "        return ep_reward\n",
    "\n",
    "    def train(self, env, batch_sz=64, updates=250):\n",
    "        # Storage helpers for a single batch of data.\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "\n",
    "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = env.reset()\n",
    "        for update in range(updates):\n",
    "            for step in range(batch_sz):\n",
    "                observations[step] = next_obs.copy()\n",
    "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "                ep_rewards[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rewards.append(0.0)\n",
    "                    next_obs = env.reset()\n",
    "                    print(\"Episode: %03d, Reward: %03d\" % (\n",
    "                        len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "            _, next_value = self.model.action_value(next_obs[None, :])\n",
    "\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "            # A trick to input actions and advantages through same API.\n",
    "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "            # Performs a full training step on the collected batch.\n",
    "            # Note: no need to mess around with gradients, Keras API handles it.\n",
    "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "\n",
    "            print(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "        returns = returns[:-1]\n",
    "\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # Value loss is typically MSE between value estimates and returns.\n",
    "        return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, actions_and_advantages, logits):\n",
    "        # A trick to input actions and advantages through the same API.\n",
    "        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "\n",
    "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "        # Note: we only calculate the loss on the actions we've actually taken.\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "\n",
    "        # Entropy loss can be calculated as cross-entropy over itself.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "\n",
    "        # We want to minimize policy and maximize entropy losses.\n",
    "        # Here signs are flipped because the optimizer minimizes.\n",
    "        return policy_loss - self.entropy_c * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001, Reward: 019\n",
      "Episode: 002, Reward: 018\n",
      "Episode: 003, Reward: 024\n",
      "[1/250] Losses: [69.9031753540039, 6.674508571624756, 63.22866439819336]\n",
      "Episode: 004, Reward: 016\n",
      "Episode: 005, Reward: 011\n",
      "[2/250] Losses: [146.6148223876953, 8.87785816192627, 137.73696899414062]\n",
      "Episode: 006, Reward: 051\n",
      "Episode: 007, Reward: 049\n",
      "[3/250] Losses: [227.6979217529297, 10.87327766418457, 216.82464599609375]\n",
      "Episode: 008, Reward: 063\n",
      "[4/250] Losses: [355.46429443359375, 14.739948272705078, 340.7243347167969]\n",
      "[5/250] Losses: [445.7047119140625, 17.5133113861084, 428.19140625]\n",
      "Episode: 009, Reward: 093\n",
      "Episode: 010, Reward: 017\n",
      "[6/250] Losses: [66.22960662841797, 6.010138511657715, 60.21946716308594]\n",
      "Episode: 011, Reward: 031\n",
      "Episode: 012, Reward: 048\n",
      "[7/250] Losses: [194.1044921875, 9.785928726196289, 184.3185577392578]\n",
      "Episode: 013, Reward: 014\n",
      "Episode: 014, Reward: 019\n",
      "Episode: 015, Reward: 018\n",
      "[8/250] Losses: [44.76246643066406, 4.429111003875732, 40.33335494995117]\n",
      "Episode: 016, Reward: 061\n",
      "[9/250] Losses: [138.80715942382812, 8.845809936523438, 129.9613494873047]\n",
      "Episode: 017, Reward: 042\n",
      "Episode: 018, Reward: 044\n",
      "[10/250] Losses: [153.3366241455078, 8.597976684570312, 144.7386474609375]\n",
      "Episode: 019, Reward: 022\n",
      "Episode: 020, Reward: 015\n",
      "Episode: 021, Reward: 017\n",
      "[11/250] Losses: [30.92755699157715, 3.7270069122314453, 27.200550079345703]\n",
      "Episode: 022, Reward: 037\n",
      "Episode: 023, Reward: 024\n",
      "[12/250] Losses: [66.46342468261719, 6.365276336669922, 60.09814453125]\n",
      "Episode: 024, Reward: 017\n",
      "Episode: 025, Reward: 044\n",
      "[13/250] Losses: [146.09974670410156, 8.448762893676758, 137.65098571777344]\n",
      "Episode: 026, Reward: 022\n",
      "Episode: 027, Reward: 022\n",
      "Episode: 028, Reward: 023\n",
      "[14/250] Losses: [40.20782470703125, 4.073543548583984, 36.134281158447266]\n",
      "Episode: 029, Reward: 038\n",
      "[15/250] Losses: [160.32180786132812, 8.784055709838867, 151.53775024414062]\n",
      "Episode: 030, Reward: 053\n",
      "Episode: 031, Reward: 024\n",
      "Episode: 032, Reward: 024\n",
      "[16/250] Losses: [39.419700622558594, 3.35677433013916, 36.06292724609375]\n",
      "Episode: 033, Reward: 034\n",
      "[17/250] Losses: [139.74623107910156, 9.199464797973633, 130.54676818847656]\n",
      "Episode: 034, Reward: 098\n",
      "[18/250] Losses: [326.6875, 12.639152526855469, 314.04833984375]\n",
      "Episode: 035, Reward: 038\n",
      "[19/250] Losses: [114.7406234741211, 7.842679500579834, 106.89794158935547]\n",
      "Episode: 036, Reward: 030\n",
      "Episode: 037, Reward: 020\n",
      "Episode: 038, Reward: 027\n",
      "[20/250] Losses: [38.78355026245117, 2.6679649353027344, 36.11558532714844]\n",
      "[21/250] Losses: [420.3697814941406, 16.10868263244629, 404.2611083984375]\n",
      "Episode: 039, Reward: 095\n",
      "Episode: 040, Reward: 028\n",
      "[22/250] Losses: [41.66897201538086, 3.8400607109069824, 37.82891082763672]\n",
      "Episode: 041, Reward: 053\n",
      "[23/250] Losses: [110.05345153808594, 6.733688831329346, 103.31976318359375]\n",
      "[24/250] Losses: [452.5729064941406, 15.397062301635742, 437.17584228515625]\n",
      "Episode: 042, Reward: 113\n",
      "[25/250] Losses: [182.58847045898438, 8.033321380615234, 174.55514526367188]\n",
      "Episode: 043, Reward: 060\n",
      "Episode: 044, Reward: 040\n",
      "[26/250] Losses: [69.31449127197266, 4.036978721618652, 65.27751159667969]\n",
      "[27/250] Losses: [397.920166015625, 14.212284088134766, 383.7078857421875]\n",
      "Episode: 045, Reward: 133\n",
      "[28/250] Losses: [267.20831298828125, 11.956867218017578, 255.25144958496094]\n",
      "[29/250] Losses: [385.06988525390625, 12.89512825012207, 372.17474365234375]\n",
      "Episode: 046, Reward: 117\n",
      "[30/250] Losses: [119.20355224609375, 5.576807022094727, 113.62674713134766]\n",
      "Episode: 047, Reward: 049\n",
      "[31/250] Losses: [89.40729522705078, 4.9296674728393555, 84.47763061523438]\n",
      "Episode: 048, Reward: 036\n",
      "Episode: 049, Reward: 025\n",
      "Episode: 050, Reward: 022\n",
      "[32/250] Losses: [23.934864044189453, 1.5331311225891113, 22.4017333984375]\n",
      "Episode: 051, Reward: 047\n",
      "[33/250] Losses: [80.3755874633789, 4.92587423324585, 75.44971466064453]\n",
      "Episode: 052, Reward: 064\n",
      "[34/250] Losses: [74.77658081054688, 4.2617034912109375, 70.51487731933594]\n",
      "Episode: 053, Reward: 043\n",
      "[35/250] Losses: [165.27235412597656, 7.780176639556885, 157.49217224121094]\n",
      "[36/250] Losses: [383.1443786621094, 12.872365951538086, 370.2720031738281]\n",
      "[37/250] Losses: [513.639404296875, 17.324783325195312, 496.31463623046875]\n",
      "Episode: 054, Reward: 185\n",
      "[38/250] Losses: [324.09857177734375, 9.572436332702637, 314.526123046875]\n",
      "Episode: 055, Reward: 063\n",
      "[39/250] Losses: [269.52484130859375, 10.908937454223633, 258.61590576171875]\n",
      "[40/250] Losses: [312.6463317871094, 12.77595329284668, 299.8703918457031]\n",
      "Episode: 056, Reward: 123\n",
      "[41/250] Losses: [279.5547790527344, 10.698566436767578, 268.856201171875]\n",
      "Episode: 057, Reward: 070\n",
      "[42/250] Losses: [193.776611328125, 7.438449859619141, 186.33816528320312]\n",
      "Episode: 058, Reward: 116\n",
      "[43/250] Losses: [142.71063232421875, 6.276581764221191, 136.43405151367188]\n",
      "Episode: 059, Reward: 063\n",
      "[44/250] Losses: [131.40452575683594, 6.38389253616333, 125.02063751220703]\n",
      "[45/250] Losses: [275.9088134765625, 12.271051406860352, 263.63775634765625]\n",
      "[46/250] Losses: [326.9283752441406, 12.37100887298584, 314.557373046875]\n",
      "[47/250] Losses: [341.3244323730469, 13.920490264892578, 327.4039306640625]\n",
      "Episode: 060, Reward: 200\n",
      "[48/250] Losses: [304.4903564453125, 11.127154350280762, 293.3631896972656]\n",
      "[49/250] Losses: [324.2352600097656, 13.301361083984375, 310.93389892578125]\n",
      "Episode: 061, Reward: 136\n",
      "[50/250] Losses: [201.8145294189453, 5.435812950134277, 196.37872314453125]\n",
      "Episode: 062, Reward: 116\n",
      "[51/250] Losses: [127.21298217773438, 6.148405075073242, 121.0645751953125]\n",
      "[52/250] Losses: [259.08331298828125, 12.561700820922852, 246.5216064453125]\n",
      "[53/250] Losses: [403.06817626953125, 16.297073364257812, 386.7710876464844]\n",
      "[54/250] Losses: [267.1668395996094, 11.77523136138916, 255.3916015625]\n",
      "Episode: 063, Reward: 200\n",
      "Episode: 064, Reward: 037\n",
      "[55/250] Losses: [63.25489807128906, -1.88653564453125, 65.14143371582031]\n",
      "[56/250] Losses: [278.0519104003906, 12.518108367919922, 265.5338134765625]\n",
      "Episode: 065, Reward: 102\n",
      "[57/250] Losses: [151.14761352539062, 3.399998903274536, 147.74761962890625]\n",
      "[58/250] Losses: [300.91021728515625, 12.265421867370605, 288.6448059082031]\n",
      "[59/250] Losses: [390.1300354003906, 15.894509315490723, 374.23553466796875]\n",
      "Episode: 066, Reward: 199\n",
      "[60/250] Losses: [210.49913024902344, 1.347191333770752, 209.1519317626953]\n",
      "[61/250] Losses: [319.4488525390625, 12.95085334777832, 306.49798583984375]\n",
      "Episode: 067, Reward: 106\n",
      "Episode: 068, Reward: 048\n",
      "[62/250] Losses: [104.27605438232422, -1.8709337711334229, 106.14698791503906]\n",
      "[63/250] Losses: [234.40232849121094, 10.196322441101074, 224.2060089111328]\n",
      "Episode: 069, Reward: 137\n",
      "[64/250] Losses: [219.3987274169922, -9.585342407226562, 228.98406982421875]\n",
      "Episode: 070, Reward: 033\n",
      "[65/250] Losses: [85.52613067626953, 1.3088233470916748, 84.2173080444336]\n",
      "[66/250] Losses: [252.90335083007812, 10.852602005004883, 242.05075073242188]\n",
      "[67/250] Losses: [310.3150634765625, 13.464534759521484, 296.85052490234375]\n",
      "Episode: 071, Reward: 200\n",
      "[68/250] Losses: [123.82315063476562, -2.371893882751465, 126.1950454711914]\n",
      "[69/250] Losses: [296.8960876464844, 14.089438438415527, 282.806640625]\n",
      "[70/250] Losses: [140.2587890625, 8.507108688354492, 131.75167846679688]\n",
      "Episode: 072, Reward: 200\n",
      "[71/250] Losses: [84.61155700683594, -3.589601993560791, 88.20115661621094]\n",
      "[72/250] Losses: [225.02236938476562, 10.703118324279785, 214.31924438476562]\n",
      "[73/250] Losses: [181.213134765625, 9.798432350158691, 171.41470336914062]\n",
      "Episode: 073, Reward: 163\n",
      "[74/250] Losses: [160.0331268310547, 4.028939247131348, 156.00418090820312]\n",
      "[75/250] Losses: [179.56297302246094, 10.694319725036621, 168.86865234375]\n",
      "Episode: 074, Reward: 132\n",
      "[76/250] Losses: [101.53439331054688, 0.6341431140899658, 100.90025329589844]\n",
      "Episode: 075, Reward: 077\n",
      "[77/250] Losses: [66.61873626708984, -3.6530520915985107, 70.27178955078125]\n",
      "Episode: 076, Reward: 068\n",
      "[78/250] Losses: [68.12468719482422, -0.5515031814575195, 68.67619323730469]\n",
      "[79/250] Losses: [191.75503540039062, 10.354240417480469, 181.40078735351562]\n",
      "[80/250] Losses: [211.8275146484375, 10.081369400024414, 201.7461395263672]\n",
      "Episode: 077, Reward: 165\n",
      "[81/250] Losses: [190.2701416015625, 4.172511100769043, 186.09762573242188]\n",
      "Episode: 078, Reward: 116\n",
      "[82/250] Losses: [100.97636413574219, -4.050716400146484, 105.02708435058594]\n",
      "[83/250] Losses: [171.81138610839844, 9.477328300476074, 162.3340606689453]\n",
      "[84/250] Losses: [208.1472625732422, 10.374593734741211, 197.77267456054688]\n",
      "Episode: 079, Reward: 174\n",
      "[85/250] Losses: [57.433250427246094, -3.0742902755737305, 60.50754165649414]\n",
      "[86/250] Losses: [211.0964813232422, 10.337349891662598, 200.75912475585938]\n",
      "[87/250] Losses: [170.97853088378906, 10.466737747192383, 160.5117950439453]\n",
      "Episode: 080, Reward: 178\n",
      "[88/250] Losses: [106.631591796875, -1.7857170104980469, 108.41731262207031]\n",
      "[89/250] Losses: [150.97711181640625, 9.012798309326172, 141.9643096923828]\n",
      "Episode: 081, Reward: 146\n",
      "[90/250] Losses: [65.1798095703125, -2.985729217529297, 68.16554260253906]\n",
      "[91/250] Losses: [181.9378204345703, 9.660455703735352, 172.27735900878906]\n",
      "[92/250] Losses: [184.02716064453125, 10.033140182495117, 173.9940185546875]\n",
      "Episode: 082, Reward: 156\n",
      "[93/250] Losses: [127.95626068115234, 2.1247758865356445, 125.83148193359375]\n",
      "[94/250] Losses: [195.05645751953125, 10.513174057006836, 184.5432891845703]\n",
      "Episode: 083, Reward: 169\n",
      "[95/250] Losses: [94.29031372070312, -5.714886665344238, 100.00520324707031]\n",
      "[96/250] Losses: [204.5074005126953, 10.50868034362793, 193.99871826171875]\n",
      "[97/250] Losses: [229.97415161132812, 11.811277389526367, 218.16287231445312]\n",
      "Episode: 084, Reward: 200\n",
      "[98/250] Losses: [178.7366943359375, -8.874201774597168, 187.61090087890625]\n",
      "[99/250] Losses: [175.5738525390625, 9.360933303833008, 166.21292114257812]\n",
      "[100/250] Losses: [134.6313018798828, 8.115629196166992, 126.51567840576172]\n",
      "Episode: 085, Reward: 166\n",
      "[101/250] Losses: [205.54525756835938, -6.575895309448242, 212.12115478515625]\n",
      "Episode: 086, Reward: 063\n",
      "[102/250] Losses: [192.5791473388672, -5.1942644119262695, 197.77340698242188]\n",
      "[103/250] Losses: [209.08770751953125, 11.132474899291992, 197.95523071289062]\n",
      "[104/250] Losses: [127.9231948852539, 8.063486099243164, 119.85971069335938]\n",
      "Episode: 087, Reward: 186\n",
      "[105/250] Losses: [196.94281005859375, -7.752861022949219, 204.6956787109375]\n",
      "[106/250] Losses: [167.0552215576172, 8.52472972869873, 158.53048706054688]\n",
      "[107/250] Losses: [150.0613555908203, 8.40373420715332, 141.65762329101562]\n",
      "Episode: 088, Reward: 200\n",
      "[108/250] Losses: [232.89402770996094, -6.668137550354004, 239.56216430664062]\n",
      "[109/250] Losses: [229.82846069335938, 11.156963348388672, 218.67149353027344]\n",
      "[110/250] Losses: [120.26258850097656, 7.153656005859375, 113.10893249511719]\n",
      "Episode: 089, Reward: 200\n",
      "[111/250] Losses: [261.4260559082031, -9.769929885864258, 271.19598388671875]\n",
      "[112/250] Losses: [137.62176513671875, 7.922015190124512, 129.6997528076172]\n",
      "[113/250] Losses: [114.2478256225586, 7.244542121887207, 107.00328063964844]\n",
      "Episode: 090, Reward: 200\n",
      "[114/250] Losses: [299.021240234375, -9.364511489868164, 308.3857421875]\n",
      "[115/250] Losses: [119.8890609741211, 5.775326728820801, 114.11373138427734]\n",
      "[116/250] Losses: [125.30999755859375, 8.112586975097656, 117.1974105834961]\n",
      "Episode: 091, Reward: 140\n",
      "[117/250] Losses: [151.6388702392578, 6.678905963897705, 144.9599609375]\n",
      "[118/250] Losses: [103.6563491821289, 6.5989274978637695, 97.05741882324219]\n",
      "[119/250] Losses: [98.68125915527344, 7.5799713134765625, 91.10128784179688]\n",
      "Episode: 092, Reward: 200\n",
      "[120/250] Losses: [191.3270721435547, 1.9132282733917236, 189.41384887695312]\n",
      "[121/250] Losses: [127.26673126220703, 7.498603820800781, 119.76812744140625]\n",
      "[122/250] Losses: [212.08509826660156, 10.776188850402832, 201.3089141845703]\n",
      "Episode: 093, Reward: 200\n",
      "[123/250] Losses: [433.00006103515625, -2.3705766201019287, 435.3706359863281]\n",
      "[124/250] Losses: [103.13098907470703, 6.554108619689941, 96.5768814086914]\n",
      "[125/250] Losses: [95.0215835571289, 6.8668437004089355, 88.15473937988281]\n",
      "Episode: 094, Reward: 200\n",
      "[126/250] Losses: [332.9286804199219, -7.311563491821289, 340.240234375]\n",
      "Episode: 095, Reward: 059\n",
      "[127/250] Losses: [295.40740966796875, -4.0724616050720215, 299.4798583984375]\n",
      "[128/250] Losses: [84.40791320800781, 5.489398002624512, 78.91851806640625]\n",
      "[129/250] Losses: [113.37847900390625, 7.515970230102539, 105.86251068115234]\n",
      "Episode: 096, Reward: 200\n",
      "[130/250] Losses: [394.3122253417969, -8.189367294311523, 402.5015869140625]\n",
      "[131/250] Losses: [121.70417022705078, 7.528019905090332, 114.1761474609375]\n",
      "[132/250] Losses: [135.14230346679688, 6.509955406188965, 128.63235473632812]\n",
      "Episode: 097, Reward: 200\n",
      "[133/250] Losses: [584.2459106445312, -11.620229721069336, 595.8661499023438]\n",
      "[134/250] Losses: [101.76602172851562, 6.663619041442871, 95.10240173339844]\n",
      "[135/250] Losses: [101.84552001953125, 6.242672920227051, 95.60284423828125]\n",
      "Episode: 098, Reward: 200\n",
      "[136/250] Losses: [449.7489929199219, -14.366071701049805, 464.11505126953125]\n",
      "[137/250] Losses: [96.04341888427734, 6.350264072418213, 89.69315338134766]\n",
      "[138/250] Losses: [93.05078125, 6.795087814331055, 86.25569152832031]\n",
      "Episode: 099, Reward: 200\n",
      "[139/250] Losses: [509.2310791015625, -14.554006576538086, 523.7850952148438]\n",
      "[140/250] Losses: [111.91481018066406, 7.763144493103027, 104.15166473388672]\n",
      "[141/250] Losses: [91.73318481445312, 6.110284805297852, 85.6229019165039]\n",
      "Episode: 100, Reward: 200\n",
      "[142/250] Losses: [497.7165832519531, -14.67067813873291, 512.3872680664062]\n",
      "[143/250] Losses: [115.05884552001953, 7.571693420410156, 107.48715209960938]\n",
      "[144/250] Losses: [89.90113067626953, 5.9551777839660645, 83.94595336914062]\n",
      "[145/250] Losses: [113.13351440429688, 8.090551376342773, 105.04296112060547]\n",
      "Episode: 101, Reward: 200\n",
      "[146/250] Losses: [164.56700134277344, 3.620344400405884, 160.9466552734375]\n",
      "[147/250] Losses: [94.73908233642578, 6.8656816482543945, 87.87339782714844]\n",
      "[148/250] Losses: [89.6982192993164, 6.469934940338135, 83.22828674316406]\n",
      "Episode: 102, Reward: 200\n",
      "[149/250] Losses: [313.9459533691406, -2.3230185508728027, 316.26898193359375]\n",
      "[150/250] Losses: [80.2022933959961, 5.507568836212158, 74.6947250366211]\n",
      "[151/250] Losses: [77.80079650878906, 5.567870140075684, 72.23292541503906]\n",
      "Episode: 103, Reward: 200\n",
      "[152/250] Losses: [465.8606262207031, -7.915876388549805, 473.7764892578125]\n",
      "[153/250] Losses: [86.65282440185547, 4.773040771484375, 81.8797836303711]\n",
      "[154/250] Losses: [55.668373107910156, 4.342150688171387, 51.32622146606445]\n",
      "Episode: 104, Reward: 200\n",
      "[155/250] Losses: [601.4300537109375, -9.13418197631836, 610.564208984375]\n",
      "[156/250] Losses: [93.94721221923828, 5.044430255889893, 88.90277862548828]\n",
      "[157/250] Losses: [55.065921783447266, 4.189548492431641, 50.876373291015625]\n",
      "Episode: 105, Reward: 200\n",
      "[158/250] Losses: [801.3475341796875, -13.401976585388184, 814.74951171875]\n",
      "[159/250] Losses: [66.38084411621094, 4.237948894500732, 62.14289855957031]\n",
      "[160/250] Losses: [53.8908576965332, 4.5630693435668945, 49.327789306640625]\n",
      "Episode: 106, Reward: 190\n",
      "[161/250] Losses: [508.3197937011719, -15.273294448852539, 523.5930786132812]\n",
      "[162/250] Losses: [65.5499038696289, 4.113359451293945, 61.436546325683594]\n",
      "[163/250] Losses: [27.678974151611328, 3.1224870681762695, 24.556486129760742]\n",
      "Episode: 107, Reward: 171\n",
      "[164/250] Losses: [266.5399475097656, -1.4681739807128906, 268.00811767578125]\n",
      "[165/250] Losses: [38.69609451293945, 3.5248920917510986, 35.17120361328125]\n",
      "[166/250] Losses: [100.1572036743164, 6.887656211853027, 93.26954650878906]\n",
      "Episode: 108, Reward: 200\n",
      "[167/250] Losses: [453.1045837402344, -4.531520843505859, 457.6361083984375]\n",
      "[168/250] Losses: [96.3680648803711, 5.675565719604492, 90.69249725341797]\n",
      "[169/250] Losses: [46.46969223022461, 3.82719087600708, 42.64250183105469]\n",
      "Episode: 109, Reward: 195\n",
      "[170/250] Losses: [628.7144165039062, -7.535472869873047, 636.2498779296875]\n",
      "[171/250] Losses: [66.48999786376953, 5.525927543640137, 60.96406936645508]\n",
      "Episode: 110, Reward: 144\n",
      "[172/250] Losses: [675.3863525390625, -13.1346435546875, 688.52099609375]\n",
      "[173/250] Losses: [91.56389617919922, 5.357675552368164, 86.20622253417969]\n",
      "[174/250] Losses: [36.80034637451172, 2.9417102336883545, 33.85863494873047]\n",
      "Episode: 111, Reward: 157\n",
      "[175/250] Losses: [218.77835083007812, 2.9512345790863037, 215.82711791992188]\n",
      "Episode: 112, Reward: 121\n",
      "[176/250] Losses: [824.4307861328125, -18.38542938232422, 842.8162231445312]\n",
      "[177/250] Losses: [84.582763671875, 5.581456184387207, 79.00130462646484]\n",
      "[178/250] Losses: [57.36339569091797, 4.959080696105957, 52.40431594848633]\n",
      "Episode: 113, Reward: 171\n",
      "[179/250] Losses: [564.0562744140625, -11.894349098205566, 575.9506225585938]\n",
      "[180/250] Losses: [69.16796112060547, 5.28951358795166, 63.878448486328125]\n",
      "[181/250] Losses: [51.533172607421875, 5.100507736206055, 46.43266296386719]\n",
      "Episode: 114, Reward: 200\n",
      "[182/250] Losses: [564.521240234375, -14.674863815307617, 579.1961059570312]\n",
      "[183/250] Losses: [110.36888122558594, 7.030551910400391, 103.33832550048828]\n",
      "Episode: 115, Reward: 135\n",
      "[184/250] Losses: [818.7939453125, -19.23200225830078, 838.0259399414062]\n",
      "[185/250] Losses: [72.92047119140625, 5.418455600738525, 67.50201416015625]\n",
      "Episode: 116, Reward: 117\n",
      "[186/250] Losses: [524.807861328125, -12.390641212463379, 537.198486328125]\n",
      "[187/250] Losses: [60.62102127075195, 5.104941368103027, 55.51607894897461]\n",
      "[188/250] Losses: [102.51085662841797, 7.113282203674316, 95.39757537841797]\n",
      "Episode: 117, Reward: 200\n",
      "[189/250] Losses: [617.76220703125, -15.073895454406738, 632.8361206054688]\n",
      "[190/250] Losses: [79.43766784667969, 6.018322944641113, 73.41934204101562]\n",
      "[191/250] Losses: [80.67411804199219, 5.77178955078125, 74.90232849121094]\n",
      "Episode: 118, Reward: 200\n",
      "[192/250] Losses: [553.3677978515625, -16.44766616821289, 569.8154907226562]\n",
      "[193/250] Losses: [68.97174835205078, 5.361739635467529, 63.610008239746094]\n",
      "[194/250] Losses: [96.91966247558594, 6.6768059730529785, 90.24285888671875]\n",
      "[195/250] Losses: [87.02449798583984, 6.2623090744018555, 80.76219177246094]\n",
      "Episode: 119, Reward: 200\n",
      "[196/250] Losses: [182.29444885253906, 2.6195216178894043, 179.6749267578125]\n",
      "[197/250] Losses: [62.33319854736328, 5.243435859680176, 57.08976364135742]\n",
      "[198/250] Losses: [77.16771697998047, 5.935511112213135, 71.23220825195312]\n",
      "Episode: 120, Reward: 200\n",
      "[199/250] Losses: [304.16156005859375, -1.957878589630127, 306.11944580078125]\n",
      "Episode: 121, Reward: 082\n",
      "[200/250] Losses: [528.1600341796875, -10.213371276855469, 538.3734130859375]\n",
      "Episode: 122, Reward: 056\n",
      "[201/250] Losses: [436.5315246582031, -7.067296504974365, 443.59881591796875]\n",
      "[202/250] Losses: [69.15910339355469, 5.821488380432129, 63.337615966796875]\n",
      "Episode: 123, Reward: 123\n",
      "[203/250] Losses: [386.374755859375, -5.990176200866699, 392.36492919921875]\n",
      "Episode: 124, Reward: 101\n",
      "[204/250] Losses: [642.4321899414062, -16.73735809326172, 659.1695556640625]\n",
      "Episode: 125, Reward: 010\n",
      "Episode: 126, Reward: 024\n",
      "Episode: 127, Reward: 025\n",
      "[205/250] Losses: [973.2769165039062, -22.056236267089844, 995.3331298828125]\n",
      "Episode: 128, Reward: 017\n",
      "Episode: 129, Reward: 013\n",
      "Episode: 130, Reward: 008\n",
      "Episode: 131, Reward: 011\n",
      "Episode: 132, Reward: 010\n",
      "Episode: 133, Reward: 011\n",
      "[206/250] Losses: [1363.497314453125, -25.57278060913086, 1389.070068359375]\n",
      "Episode: 134, Reward: 010\n",
      "Episode: 135, Reward: 010\n",
      "Episode: 136, Reward: 019\n",
      "Episode: 137, Reward: 018\n",
      "Episode: 138, Reward: 013\n",
      "[207/250] Losses: [1153.2086181640625, -30.01177215576172, 1183.2203369140625]\n",
      "Episode: 139, Reward: 015\n",
      "Episode: 140, Reward: 017\n",
      "[208/250] Losses: [515.3960571289062, -10.288107872009277, 525.6841430664062]\n",
      "Episode: 141, Reward: 042\n",
      "Episode: 142, Reward: 018\n",
      "Episode: 143, Reward: 021\n",
      "Episode: 144, Reward: 013\n",
      "[209/250] Losses: [949.5200805664062, -20.481338500976562, 970.0014038085938]\n",
      "Episode: 145, Reward: 016\n",
      "Episode: 146, Reward: 012\n",
      "Episode: 147, Reward: 013\n",
      "Episode: 148, Reward: 010\n",
      "Episode: 149, Reward: 016\n",
      "[210/250] Losses: [966.3009643554688, -24.64065933227539, 990.941650390625]\n",
      "Episode: 150, Reward: 015\n",
      "Episode: 151, Reward: 014\n",
      "Episode: 152, Reward: 012\n",
      "Episode: 153, Reward: 018\n",
      "[211/250] Losses: [769.001220703125, -22.491111755371094, 791.4923095703125]\n",
      "[212/250] Losses: [142.02822875976562, 7.5609917640686035, 134.4672393798828]\n",
      "Episode: 154, Reward: 117\n",
      "[213/250] Losses: [368.14593505859375, -9.650423049926758, 377.7963562011719]\n",
      "[214/250] Losses: [99.1241226196289, 6.7647480964660645, 92.359375]\n",
      "Episode: 155, Reward: 118\n",
      "Episode: 156, Reward: 016\n",
      "[215/250] Losses: [446.24029541015625, -16.868480682373047, 463.1087646484375]\n",
      "[216/250] Losses: [102.68798065185547, 6.66970157623291, 96.01828002929688]\n",
      "[217/250] Losses: [130.55303955078125, 8.152778625488281, 122.4002685546875]\n",
      "Episode: 157, Reward: 200\n",
      "[218/250] Losses: [154.2526397705078, -7.316216945648193, 161.56886291503906]\n",
      "[219/250] Losses: [102.830078125, 6.661177635192871, 96.16889953613281]\n",
      "[220/250] Losses: [119.08573150634766, 7.411391258239746, 111.6743392944336]\n",
      "Episode: 158, Reward: 177\n",
      "[221/250] Losses: [191.1978302001953, -9.22508716583252, 200.42291259765625]\n",
      "[222/250] Losses: [84.17902374267578, 4.960273742675781, 79.21875]\n",
      "[223/250] Losses: [127.3536605834961, 7.956151008605957, 119.39750671386719]\n",
      "Episode: 159, Reward: 200\n",
      "[224/250] Losses: [210.05335998535156, -9.115537643432617, 219.1688995361328]\n",
      "[225/250] Losses: [117.83255767822266, 8.22734546661377, 109.60520935058594]\n",
      "[226/250] Losses: [91.27458190917969, 7.029018402099609, 84.24555969238281]\n",
      "Episode: 160, Reward: 200\n",
      "[227/250] Losses: [204.66567993164062, -8.083335876464844, 212.74900817871094]\n",
      "[228/250] Losses: [120.8058853149414, 6.717271327972412, 114.08861541748047]\n",
      "[229/250] Losses: [98.39092254638672, 6.715080261230469, 91.67584228515625]\n",
      "[230/250] Losses: [132.98072814941406, 8.512980461120605, 124.46774291992188]\n",
      "Episode: 161, Reward: 200\n",
      "[231/250] Losses: [147.61424255371094, 2.6051855087280273, 145.00906372070312]\n",
      "[232/250] Losses: [80.60870361328125, 5.776185989379883, 74.83251953125]\n",
      "[233/250] Losses: [161.25799560546875, 8.869010925292969, 152.38897705078125]\n",
      "Episode: 162, Reward: 200\n",
      "[234/250] Losses: [210.640625, -0.7627353072166443, 211.4033660888672]\n",
      "[235/250] Losses: [90.91120910644531, 6.793505668640137, 84.11770629882812]\n",
      "[236/250] Losses: [98.53162384033203, 6.47733211517334, 92.05429077148438]\n",
      "Episode: 163, Reward: 200\n",
      "[237/250] Losses: [251.06214904785156, -3.0201590061187744, 254.08230590820312]\n",
      "[238/250] Losses: [35.72005081176758, 3.6939339637756348, 32.02611541748047]\n",
      "Episode: 164, Reward: 123\n",
      "[239/250] Losses: [283.2392272949219, -1.276611328125, 284.5158386230469]\n",
      "[240/250] Losses: [36.173362731933594, 3.70340633392334, 32.46995544433594]\n",
      "Episode: 165, Reward: 128\n",
      "[241/250] Losses: [254.8479461669922, -1.9664192199707031, 256.8143615722656]\n",
      "[242/250] Losses: [58.63713836669922, 5.434418678283691, 53.202720642089844]\n",
      "Episode: 166, Reward: 115\n",
      "[243/250] Losses: [182.8081512451172, 4.259164333343506, 178.54898071289062]\n",
      "Episode: 167, Reward: 110\n",
      "[244/250] Losses: [533.4107055664062, -15.490835189819336, 548.9015502929688]\n",
      "Episode: 168, Reward: 048\n",
      "[245/250] Losses: [462.2974853515625, -9.250675201416016, 471.54815673828125]\n",
      "Episode: 169, Reward: 074\n",
      "[246/250] Losses: [413.702392578125, -9.273497581481934, 422.97589111328125]\n",
      "Episode: 170, Reward: 029\n",
      "Episode: 171, Reward: 024\n",
      "Episode: 172, Reward: 017\n",
      "[247/250] Losses: [725.5036010742188, -20.0135498046875, 745.5171508789062]\n",
      "Episode: 173, Reward: 078\n",
      "[248/250] Losses: [323.67840576171875, -12.037723541259766, 335.71612548828125]\n",
      "Episode: 174, Reward: 051\n",
      "[249/250] Losses: [339.0235595703125, -11.647478103637695, 350.6710510253906]\n",
      "[250/250] Losses: [111.9266357421875, 7.368610382080078, 104.55802154541016]\n",
      "Finished training, testing...\n",
      "114 out of 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = A2C(model)\n",
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training, testing...\")\n",
    "print(\"%d out of 200\" % agent.test(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Episode: 001, Reward: 017\n",
      "Episode: 002, Reward: 026\n",
      "[1/250] Losses: [82.02988, 7.392539, 74.63734]\n",
      "Episode: 003, Reward: 023\n",
      "Episode: 004, Reward: 042\n",
      "Episode: 005, Reward: 015\n",
      "[2/250] Losses: [162.14333, 9.606596, 152.53673]\n",
      "Episode: 006, Reward: 011\n",
      "Episode: 007, Reward: 010\n",
      "Episode: 008, Reward: 022\n",
      "Episode: 009, Reward: 012\n",
      "[3/250] Losses: [42.31421, 4.928278, 37.385933]\n",
      "Episode: 010, Reward: 022\n",
      "Episode: 011, Reward: 011\n",
      "Episode: 012, Reward: 012\n",
      "[4/250] Losses: [89.80336, 6.76303, 83.04033]\n",
      "Episode: 013, Reward: 041\n",
      "Episode: 014, Reward: 022\n",
      "[5/250] Losses: [115.04283, 8.0042305, 107.038605]\n",
      "Episode: 015, Reward: 035\n",
      "Episode: 016, Reward: 013\n",
      "[6/250] Losses: [247.10014, 11.977857, 235.12228]\n",
      "Episode: 017, Reward: 052\n",
      "Episode: 018, Reward: 032\n",
      "Episode: 019, Reward: 012\n",
      "[7/250] Losses: [83.11281, 6.5223646, 76.59045]\n",
      "Episode: 020, Reward: 070\n",
      "[8/250] Losses: [241.58469, 12.514875, 229.06981]\n",
      "Episode: 021, Reward: 029\n",
      "Episode: 022, Reward: 017\n",
      "[9/250] Losses: [85.89472, 6.7527375, 79.14198]\n",
      "Episode: 023, Reward: 052\n",
      "Episode: 024, Reward: 042\n",
      "[10/250] Losses: [144.93875, 8.73423, 136.20453]\n",
      "Episode: 025, Reward: 014\n",
      "Episode: 026, Reward: 043\n",
      "[11/250] Losses: [136.23462, 8.140903, 128.09372]\n",
      "Episode: 027, Reward: 028\n",
      "[12/250] Losses: [186.50684, 10.127205, 176.37964]\n",
      "Episode: 028, Reward: 074\n",
      "[13/250] Losses: [132.06685, 8.840189, 123.226654]\n",
      "Episode: 029, Reward: 037\n",
      "Episode: 030, Reward: 021\n",
      "Episode: 031, Reward: 036\n",
      "[14/250] Losses: [83.449005, 5.382445, 78.06656]\n",
      "Episode: 032, Reward: 032\n",
      "[15/250] Losses: [122.219894, 7.8331146, 114.38678]\n",
      "Episode: 033, Reward: 085\n",
      "[16/250] Losses: [178.4774, 9.142992, 169.33441]\n",
      "Episode: 034, Reward: 031\n",
      "[17/250] Losses: [184.75133, 9.481579, 175.26974]\n",
      "Episode: 035, Reward: 059\n",
      "Episode: 036, Reward: 050\n",
      "[18/250] Losses: [168.38403, 8.6626625, 159.72137]\n",
      "[19/250] Losses: [409.42114, 16.17857, 393.24258]\n",
      "Episode: 037, Reward: 082\n",
      "Episode: 038, Reward: 014\n",
      "[20/250] Losses: [76.02242, 4.9887652, 71.03366]\n",
      "Episode: 039, Reward: 054\n",
      "Episode: 040, Reward: 033\n",
      "[21/250] Losses: [52.96871, 4.4719844, 48.496727]\n",
      "Episode: 041, Reward: 073\n",
      "[22/250] Losses: [273.86456, 11.856083, 262.00848]\n",
      "Episode: 042, Reward: 049\n",
      "[23/250] Losses: [128.96336, 6.822724, 122.14063]\n",
      "[24/250] Losses: [397.71017, 15.510273, 382.1999]\n",
      "[25/250] Losses: [414.79407, 15.806463, 398.9876]\n",
      "Episode: 043, Reward: 149\n",
      "Episode: 044, Reward: 028\n",
      "[26/250] Losses: [78.40964, 5.4602222, 72.94942]\n",
      "Episode: 045, Reward: 039\n",
      "[27/250] Losses: [263.2023, 11.387534, 251.81476]\n",
      "Episode: 046, Reward: 064\n",
      "[28/250] Losses: [279.90805, 11.359362, 268.54868]\n",
      "Episode: 047, Reward: 060\n",
      "Episode: 048, Reward: 059\n",
      "[29/250] Losses: [188.34488, 8.046898, 180.29797]\n",
      "[30/250] Losses: [386.67636, 14.557955, 372.1184]\n",
      "Episode: 049, Reward: 078\n",
      "Episode: 050, Reward: 035\n",
      "[31/250] Losses: [41.083282, 1.8870766, 39.196205]\n",
      "Episode: 051, Reward: 044\n",
      "[32/250] Losses: [96.86786, 5.5635962, 91.30426]\n",
      "Episode: 052, Reward: 098\n",
      "[33/250] Losses: [180.4191, 8.139254, 172.27985]\n",
      "[34/250] Losses: [401.1257, 14.645514, 386.4802]\n",
      "[35/250] Losses: [353.8041, 13.274065, 340.53003]\n",
      "Episode: 053, Reward: 180\n",
      "[36/250] Losses: [71.98182, 1.7857556, 70.19606]\n",
      "Episode: 054, Reward: 059\n",
      "[37/250] Losses: [66.18646, 3.730503, 62.455956]\n",
      "[38/250] Losses: [350.5434, 14.571858, 335.97153]\n",
      "Episode: 055, Reward: 108\n",
      "Episode: 056, Reward: 020\n",
      "[39/250] Losses: [32.686028, -0.8304734, 33.516502]\n",
      "[40/250] Losses: [347.1609, 13.530045, 333.63086]\n",
      "[41/250] Losses: [339.44296, 12.967312, 326.47565]\n",
      "Episode: 057, Reward: 181\n",
      "[42/250] Losses: [94.13039, 0.78201795, 93.348366]\n",
      "Episode: 058, Reward: 045\n",
      "[43/250] Losses: [170.08192, 6.178834, 163.90309]\n",
      "[44/250] Losses: [323.31848, 15.104748, 308.21375]\n",
      "Episode: 059, Reward: 159\n",
      "[45/250] Losses: [56.25607, 1.7806242, 54.475445]\n",
      "[46/250] Losses: [320.1593, 12.609697, 307.5496]\n",
      "[47/250] Losses: [320.92087, 13.062841, 307.85803]\n",
      "Episode: 060, Reward: 200\n",
      "[48/250] Losses: [65.99041, 1.7546736, 64.23573]\n",
      "[49/250] Losses: [323.93097, 13.267883, 310.6631]\n",
      "Episode: 061, Reward: 103\n",
      "[50/250] Losses: [85.13108, 2.3340077, 82.79707]\n",
      "[51/250] Losses: [292.9001, 12.294554, 280.60553]\n",
      "[52/250] Losses: [302.70798, 13.008097, 289.6999]\n",
      "Episode: 062, Reward: 200\n",
      "[53/250] Losses: [60.363934, 0.78453916, 59.579395]\n",
      "[54/250] Losses: [282.35416, 12.08987, 270.26428]\n",
      "Episode: 063, Reward: 137\n",
      "[55/250] Losses: [50.91425, -0.74419916, 51.658447]\n",
      "[56/250] Losses: [279.75336, 11.772001, 267.98135]\n",
      "[57/250] Losses: [270.47324, 11.569864, 258.90338]\n",
      "Episode: 064, Reward: 200\n",
      "[58/250] Losses: [64.48914, -0.7223426, 65.21149]\n",
      "[59/250] Losses: [270.5991, 11.576937, 259.02216]\n",
      "[60/250] Losses: [261.3219, 10.78177, 250.54013]\n",
      "Episode: 065, Reward: 200\n",
      "[61/250] Losses: [85.57392, -3.013104, 88.58703]\n",
      "[62/250] Losses: [265.07004, 10.058474, 255.01155]\n",
      "[63/250] Losses: [339.3769, 14.100033, 325.27686]\n",
      "Episode: 066, Reward: 156\n",
      "[64/250] Losses: [266.2045, -2.8943975, 269.0989]\n",
      "[65/250] Losses: [281.01834, 10.196645, 270.8217]\n",
      "Episode: 067, Reward: 106\n",
      "[66/250] Losses: [268.5005, 7.9956064, 260.50488]\n",
      "[67/250] Losses: [227.03793, 9.120589, 217.91734]\n",
      "Episode: 068, Reward: 160\n",
      "[68/250] Losses: [234.68289, -5.8405685, 240.52347]\n",
      "[69/250] Losses: [243.87184, 9.489518, 234.38232]\n",
      "[70/250] Losses: [232.0037, 11.590333, 220.41336]\n",
      "Episode: 069, Reward: 200\n",
      "[71/250] Losses: [181.41634, -7.7123833, 189.12872]\n",
      "[72/250] Losses: [241.20938, 10.3191185, 230.89026]\n",
      "[73/250] Losses: [221.80135, 8.75712, 213.04422]\n",
      "Episode: 070, Reward: 200\n",
      "[74/250] Losses: [148.19583, -5.727319, 153.92316]\n",
      "[75/250] Losses: [247.53189, 10.319712, 237.21217]\n",
      "[76/250] Losses: [208.08594, 7.9534435, 200.13249]\n",
      "Episode: 071, Reward: 185\n",
      "[77/250] Losses: [250.45712, -5.9396133, 256.39673]\n",
      "[78/250] Losses: [212.79944, 9.084427, 203.71501]\n",
      "[79/250] Losses: [208.93617, 10.654555, 198.28162]\n",
      "Episode: 072, Reward: 200\n",
      "[80/250] Losses: [181.20282, -6.025365, 187.22818]\n",
      "[81/250] Losses: [186.6682, 10.374913, 176.29329]\n",
      "[82/250] Losses: [211.71529, 9.307608, 202.40768]\n",
      "Episode: 073, Reward: 200\n",
      "[83/250] Losses: [130.83508, -4.285551, 135.12064]\n",
      "[84/250] Losses: [195.39114, 9.171747, 186.21939]\n",
      "[85/250] Losses: [193.90128, 9.123564, 184.77771]\n",
      "[86/250] Losses: [187.81393, 8.638264, 179.17567]\n",
      "Episode: 074, Reward: 200\n",
      "[87/250] Losses: [199.79633, 7.2531824, 192.54314]\n",
      "[88/250] Losses: [167.96907, 8.215282, 159.75378]\n",
      "[89/250] Losses: [166.66408, 8.127859, 158.53622]\n",
      "Episode: 075, Reward: 200\n",
      "[90/250] Losses: [202.94641, 1.7142649, 201.23215]\n",
      "[91/250] Losses: [174.83276, 8.886833, 165.94592]\n",
      "[92/250] Losses: [181.76546, 8.940811, 172.82465]\n",
      "Episode: 076, Reward: 200\n",
      "[93/250] Losses: [369.39957, -2.6554751, 372.05505]\n",
      "[94/250] Losses: [157.85876, 6.9928055, 150.86597]\n",
      "[95/250] Losses: [151.28957, 7.868732, 143.42084]\n",
      "Episode: 077, Reward: 200\n",
      "[96/250] Losses: [236.42834, -4.1644354, 240.59277]\n",
      "[97/250] Losses: [149.3287, 7.1268573, 142.20184]\n",
      "[98/250] Losses: [165.72119, 9.942411, 155.77878]\n",
      "Episode: 078, Reward: 200\n",
      "[99/250] Losses: [291.47598, -8.864553, 300.34055]\n",
      "[100/250] Losses: [147.27545, 7.655242, 139.62021]\n",
      "[101/250] Losses: [150.6031, 7.066619, 143.53648]\n",
      "Episode: 079, Reward: 158\n",
      "[102/250] Losses: [179.09666, 6.3025417, 172.79413]\n",
      "[103/250] Losses: [145.99652, 8.12977, 137.86674]\n",
      "[104/250] Losses: [129.11365, 7.392831, 121.72081]\n",
      "Episode: 080, Reward: 200\n",
      "[105/250] Losses: [226.33922, 0.778457, 225.56076]\n",
      "[106/250] Losses: [108.198006, 6.4374743, 101.76053]\n",
      "[107/250] Losses: [115.99128, 6.205054, 109.786224]\n",
      "Episode: 081, Reward: 200\n",
      "[108/250] Losses: [336.20746, -3.9372616, 340.1447]\n",
      "[109/250] Losses: [128.32147, 6.7199574, 121.60151]\n",
      "[110/250] Losses: [110.12973, 6.2977734, 103.831955]\n",
      "Episode: 082, Reward: 200\n",
      "[111/250] Losses: [392.1189, -4.5863705, 396.70526]\n",
      "[112/250] Losses: [105.64209, 6.221667, 99.420425]\n",
      "[113/250] Losses: [102.01447, 6.27796, 95.73651]\n",
      "Episode: 083, Reward: 200\n",
      "[114/250] Losses: [427.13086, -6.769017, 433.89987]\n",
      "[115/250] Losses: [88.66138, 5.4247437, 83.23663]\n",
      "[116/250] Losses: [117.27425, 6.7381134, 110.53613]\n",
      "Episode: 084, Reward: 200\n",
      "[117/250] Losses: [475.01868, -11.072422, 486.0911]\n",
      "[118/250] Losses: [96.528885, 5.975899, 90.552986]\n",
      "[119/250] Losses: [103.84034, 5.913137, 97.9272]\n",
      "Episode: 085, Reward: 200\n",
      "[120/250] Losses: [462.33365, -12.934026, 475.26767]\n",
      "[121/250] Losses: [116.73017, 7.145666, 109.5845]\n",
      "[122/250] Losses: [107.841415, 6.4701843, 101.37123]\n",
      "Episode: 086, Reward: 200\n",
      "[123/250] Losses: [437.64658, -13.395468, 451.04205]\n",
      "[124/250] Losses: [86.93756, 6.356084, 80.581474]\n",
      "Episode: 087, Reward: 127\n",
      "[125/250] Losses: [448.08145, -18.62556, 466.707]\n",
      "[126/250] Losses: [113.18788, 6.861698, 106.32618]\n",
      "[127/250] Losses: [106.70662, 7.4216304, 99.28499]\n",
      "[128/250] Losses: [103.850815, 6.0758467, 97.77497]\n",
      "Episode: 088, Reward: 200\n",
      "[129/250] Losses: [122.70278, 5.7631307, 116.93965]\n",
      "[130/250] Losses: [101.1562, 6.40705, 94.749146]\n",
      "[131/250] Losses: [87.97304, 5.943229, 82.02981]\n",
      "Episode: 089, Reward: 200\n",
      "[132/250] Losses: [238.55219, 1.1733183, 237.37886]\n",
      "[133/250] Losses: [104.23272, 6.837119, 97.3956]\n",
      "[134/250] Losses: [71.99466, 5.555267, 66.43939]\n",
      "Episode: 090, Reward: 200\n",
      "[135/250] Losses: [373.31094, -3.9234788, 377.23444]\n",
      "[136/250] Losses: [87.27893, 6.414884, 80.864044]\n",
      "[137/250] Losses: [70.0455, 5.1721525, 64.87335]\n",
      "Episode: 091, Reward: 200\n",
      "[138/250] Losses: [511.56927, -7.985158, 519.55444]\n",
      "[139/250] Losses: [85.37875, 6.047471, 79.33128]\n",
      "[140/250] Losses: [72.69302, 5.1024213, 67.5906]\n",
      "Episode: 092, Reward: 200\n",
      "[141/250] Losses: [812.5581, -12.618137, 825.1763]\n",
      "[142/250] Losses: [82.20884, 5.330804, 76.87804]\n",
      "[143/250] Losses: [83.66909, 6.1313705, 77.53772]\n",
      "Episode: 093, Reward: 200\n",
      "[144/250] Losses: [602.2816, -13.175627, 615.4572]\n",
      "[145/250] Losses: [80.16895, 5.3201046, 74.84885]\n",
      "[146/250] Losses: [90.39476, 6.815536, 83.57922]\n",
      "Episode: 094, Reward: 200\n",
      "[147/250] Losses: [601.5953, -15.861619, 617.4569]\n",
      "[148/250] Losses: [76.160645, 5.788678, 70.37196]\n",
      "[149/250] Losses: [97.82647, 6.454796, 91.37167]\n",
      "Episode: 095, Reward: 200\n",
      "[150/250] Losses: [574.5027, -15.540314, 590.043]\n",
      "[151/250] Losses: [65.890434, 5.000284, 60.890152]\n",
      "[152/250] Losses: [108.53323, 6.6528354, 101.8804]\n",
      "[153/250] Losses: [96.84702, 6.274666, 90.57236]\n",
      "Episode: 096, Reward: 200\n",
      "[154/250] Losses: [90.13274, 4.700361, 85.43237]\n",
      "[155/250] Losses: [88.70606, 6.761528, 81.944534]\n",
      "[156/250] Losses: [69.073845, 5.5175247, 63.55632]\n",
      "Episode: 097, Reward: 200\n",
      "[157/250] Losses: [263.68826, 1.2317281, 262.45654]\n",
      "[158/250] Losses: [93.59735, 6.755053, 86.8423]\n",
      "[159/250] Losses: [51.176937, 4.1322136, 47.044724]\n",
      "Episode: 098, Reward: 200\n",
      "[160/250] Losses: [392.09384, -4.928625, 397.02246]\n",
      "[161/250] Losses: [92.13787, 6.036864, 86.101006]\n",
      "[162/250] Losses: [49.54616, 4.636753, 44.90941]\n",
      "Episode: 099, Reward: 200\n",
      "[163/250] Losses: [452.351, -8.969974, 461.32098]\n",
      "[164/250] Losses: [113.90026, 7.3740134, 106.526245]\n",
      "[165/250] Losses: [33.41089, 3.27583, 30.13506]\n",
      "Episode: 100, Reward: 200\n",
      "[166/250] Losses: [546.3405, -11.007175, 557.3477]\n",
      "[167/250] Losses: [35.580418, 3.4621582, 32.11826]\n",
      "[168/250] Losses: [64.96309, 4.1681, 60.79499]\n",
      "Episode: 101, Reward: 200\n",
      "[169/250] Losses: [644.83594, -14.348646, 659.1846]\n",
      "[170/250] Losses: [125.41807, 7.343984, 118.07408]\n",
      "[171/250] Losses: [48.368343, 5.047815, 43.32053]\n",
      "Episode: 102, Reward: 200\n",
      "[172/250] Losses: [796.8819, -18.903877, 815.78577]\n",
      "[173/250] Losses: [121.41466, 7.9964876, 113.41817]\n",
      "Episode: 103, Reward: 110\n",
      "[174/250] Losses: [699.7981, -14.883842, 714.68195]\n",
      "[175/250] Losses: [89.689, 6.4661913, 83.22281]\n",
      "[176/250] Losses: [67.31678, 5.073987, 62.24279]\n",
      "Episode: 104, Reward: 200\n",
      "[177/250] Losses: [839.5282, -15.528908, 855.0571]\n",
      "[178/250] Losses: [85.804436, 5.613645, 80.19079]\n",
      "[179/250] Losses: [80.58196, 5.180476, 75.40149]\n",
      "Episode: 105, Reward: 171\n",
      "[180/250] Losses: [652.1428, -7.346303, 659.48914]\n",
      "[181/250] Losses: [72.12138, 5.351073, 66.77031]\n",
      "[182/250] Losses: [66.86445, 5.196501, 61.66795]\n",
      "Episode: 106, Reward: 184\n",
      "[183/250] Losses: [414.89584, -0.32859528, 415.22443]\n",
      "[184/250] Losses: [80.09072, 5.07255, 75.01817]\n",
      "Episode: 107, Reward: 133\n",
      "[185/250] Losses: [573.9744, -5.4471674, 579.4216]\n",
      "Episode: 108, Reward: 094\n",
      "[186/250] Losses: [796.4236, -16.57151, 812.9951]\n",
      "Episode: 109, Reward: 047\n",
      "Episode: 110, Reward: 028\n",
      "[187/250] Losses: [1024.0835, -22.838673, 1046.9221]\n",
      "Episode: 111, Reward: 069\n",
      "[188/250] Losses: [610.3377, -14.369601, 624.70734]\n",
      "[189/250] Losses: [89.01758, 5.735924, 83.281654]\n",
      "Episode: 112, Reward: 069\n",
      "Episode: 113, Reward: 049\n",
      "[190/250] Losses: [592.5719, -15.283924, 607.85583]\n",
      "[191/250] Losses: [88.19611, 5.816778, 82.37933]\n",
      "Episode: 114, Reward: 110\n",
      "[192/250] Losses: [461.54395, -8.090048, 469.634]\n",
      "Episode: 115, Reward: 046\n",
      "[193/250] Losses: [295.28992, -1.5163183, 296.80624]\n",
      "Episode: 116, Reward: 084\n",
      "[194/250] Losses: [403.2668, -7.3336854, 410.6005]\n",
      "Episode: 117, Reward: 091\n",
      "[195/250] Losses: [387.17175, -11.814594, 398.98636]\n",
      "[196/250] Losses: [88.92836, 6.292246, 82.636116]\n",
      "[197/250] Losses: [77.31682, 5.0842094, 72.232605]\n",
      "Episode: 118, Reward: 156\n",
      "[198/250] Losses: [273.5333, -5.000346, 278.53363]\n",
      "[199/250] Losses: [84.513504, 5.6008644, 78.91264]\n",
      "Episode: 119, Reward: 105\n",
      "[200/250] Losses: [104.76906, 3.3156612, 101.4534]\n",
      "Episode: 120, Reward: 119\n",
      "[201/250] Losses: [292.0054, -9.217272, 301.2227]\n",
      "[202/250] Losses: [82.378685, 4.9370203, 77.441666]\n",
      "[203/250] Losses: [94.43253, 5.69081, 88.74172]\n",
      "Episode: 121, Reward: 140\n",
      "[204/250] Losses: [133.73616, 2.3399897, 131.39616]\n",
      "[205/250] Losses: [88.49085, 5.8958964, 82.594955]\n",
      "Episode: 122, Reward: 184\n",
      "[206/250] Losses: [284.769, -9.788605, 294.55762]\n",
      "[207/250] Losses: [95.07143, 6.33883, 88.7326]\n",
      "[208/250] Losses: [71.29083, 5.295101, 65.99573]\n",
      "[209/250] Losses: [80.339584, 5.1496615, 75.189926]\n",
      "Episode: 123, Reward: 200\n",
      "[210/250] Losses: [140.70401, 2.5671406, 138.13687]\n",
      "[211/250] Losses: [84.674225, 5.887699, 78.78652]\n",
      "[212/250] Losses: [65.012085, 4.723271, 60.28881]\n",
      "Episode: 124, Reward: 200\n",
      "[213/250] Losses: [200.20068, -0.7584635, 200.95915]\n",
      "[214/250] Losses: [66.02922, 4.783168, 61.24605]\n",
      "[215/250] Losses: [81.193054, 5.498966, 75.69409]\n",
      "Episode: 125, Reward: 200\n",
      "[216/250] Losses: [358.80225, -5.552906, 364.35516]\n",
      "[217/250] Losses: [66.89817, 5.582965, 61.31521]\n",
      "Episode: 126, Reward: 152\n",
      "[218/250] Losses: [302.12988, -10.266485, 312.39636]\n",
      "[219/250] Losses: [33.280663, 3.059603, 30.22106]\n",
      "Episode: 127, Reward: 107\n",
      "[220/250] Losses: [220.56174, -5.018074, 225.57982]\n",
      "[221/250] Losses: [46.379517, 3.5631723, 42.816345]\n",
      "Episode: 128, Reward: 121\n",
      "[222/250] Losses: [140.84744, -1.9241663, 142.7716]\n",
      "[223/250] Losses: [89.18011, 5.8249197, 83.35519]\n",
      "Episode: 129, Reward: 123\n",
      "[224/250] Losses: [126.49707, -0.4499436, 126.947014]\n",
      "Episode: 130, Reward: 094\n",
      "[225/250] Losses: [304.8511, -8.092179, 312.9433]\n",
      "Episode: 131, Reward: 060\n",
      "[226/250] Losses: [338.75418, -10.154218, 348.9084]\n",
      "Episode: 132, Reward: 058\n",
      "Episode: 133, Reward: 027\n",
      "[227/250] Losses: [771.0283, -22.202843, 793.23114]\n",
      "[228/250] Losses: [28.785183, 3.1952868, 25.589897]\n",
      "Episode: 134, Reward: 115\n",
      "[229/250] Losses: [69.00989, -4.2028465, 73.21274]\n",
      "Episode: 135, Reward: 080\n",
      "[230/250] Losses: [249.78755, -10.606064, 260.39362]\n",
      "[231/250] Losses: [31.837223, 3.313591, 28.523632]\n",
      "Episode: 136, Reward: 101\n",
      "[232/250] Losses: [82.415665, -3.1861877, 85.60185]\n",
      "[233/250] Losses: [37.951763, 3.6699781, 34.281784]\n",
      "[234/250] Losses: [45.54629, 4.281634, 41.264656]\n",
      "Episode: 137, Reward: 159\n",
      "[235/250] Losses: [58.5381, 4.477363, 54.060738]\n",
      "[236/250] Losses: [49.58387, 3.990198, 45.593674]\n",
      "Episode: 138, Reward: 148\n",
      "[237/250] Losses: [72.79756, 0.23129088, 72.56627]\n",
      "[238/250] Losses: [46.534935, 3.6638014, 42.87113]\n",
      "[239/250] Losses: [51.599037, 4.437458, 47.16158]\n",
      "Episode: 139, Reward: 178\n",
      "[240/250] Losses: [70.61765, 2.2140632, 68.40359]\n",
      "[241/250] Losses: [69.01574, 4.6138673, 64.40187]\n",
      "[242/250] Losses: [49.2649, 4.54344, 44.721462]\n",
      "Episode: 140, Reward: 200\n",
      "[243/250] Losses: [76.48179, -0.5530093, 77.0348]\n",
      "[244/250] Losses: [55.820915, 5.6859894, 50.134926]\n",
      "[245/250] Losses: [47.993576, 4.0657268, 43.92785]\n",
      "Episode: 141, Reward: 189\n",
      "[246/250] Losses: [87.31217, -0.26365244, 87.57582]\n",
      "[247/250] Losses: [89.65814, 5.9555006, 83.702644]\n",
      "[248/250] Losses: [62.146843, 5.437689, 56.709156]\n",
      "Episode: 142, Reward: 200\n",
      "[249/250] Losses: [213.544, -3.6365275, 217.18053]\n",
      "[250/250] Losses: [75.10322, 5.468467, 69.63475]\n",
      "Finished training, testing...\n",
      "200 out of 200\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    print(tf.executing_eagerly()) # False\n",
    "\n",
    "    model = Model(num_actions=env.action_space.n)\n",
    "    agent = A2C(model)\n",
    "\n",
    "    rewards_history = agent.train(env)\n",
    "    print(\"Finished training, testing...\")\n",
    "    print(\"%d out of 200\" % agent.test(env)) # 200 out of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-44a479156ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mA2C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Parallel environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/stable_baselines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mACER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macktr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mACKTR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/stable_baselines/deepq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCnnPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLnMlpPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLnCnnPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_train\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/stable_baselines/deepq/policies.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "# Test of A2C\n",
    "# obs = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     env.render()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBmVW4sZgqiky8tRA+zGaB",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ActorCritic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
